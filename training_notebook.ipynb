{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e780bbb1",
   "metadata": {},
   "source": [
    "# php "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015df6f",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d077231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a091c3",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b15e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_BASE = \"../dataset/php_cwe_079_samples\"\n",
    "    GOOD_DIR = os.path.join(DATASET_BASE, \"good\")  \n",
    "    BAD_DIR = os.path.join(DATASET_BASE, \"bad\")    \n",
    "    OUTPUT_DIR = \"./php_cwe79_codebert_output\"\n",
    "    CACHE_DIR = \"./cache\"\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"microsoft/codebert-base\"\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 8  \n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 20\n",
    "    WARMUP_RATIO = 0.1\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Data splits\n",
    "    TEST_SIZE = 0.15\n",
    "    VAL_SIZE = 0.15  # Of remaining training data\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc270d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed57762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_php_code(content: str, remove_comments: bool = True) -> str:\n",
    "    # Remove HTML comment blocks (license header)\n",
    "    if remove_comments:\n",
    "        content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove the //flaw marker (label leakage prevention)\n",
    "    content = re.sub(r'//flaw\\s*\\n?', '', content)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    content = re.sub(r'\\n\\s*\\n+', '\\n\\n', content.strip())\n",
    "    \n",
    "    return content\n",
    "\n",
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"Normalize code to detect structural duplicates.\"\"\"\n",
    "    # 1. Variables: $name -> $VAR\n",
    "    # Protect superglobals\n",
    "    superglobals = ['$_GET', '$_POST', '$_COOKIE', '$_REQUEST', '$_SERVER', '$_FILES', '$GLOBALS']\n",
    "    for i, sg in enumerate(superglobals):\n",
    "        code = code.replace(sg, f\"__SUPERGLOBAL_{i}__\")\n",
    "    \n",
    "    # Replace other variables\n",
    "    code = re.sub(r'\\$[a-zA-Z_\\x7f-\\xff][a-zA-Z0-9_\\x7f-\\xff]*', '$VAR', code)\n",
    "    \n",
    "    # Restore superglobals\n",
    "    for i, sg in enumerate(superglobals):\n",
    "        code = code.replace(f\"__SUPERGLOBAL_{i}__\", sg)\n",
    "\n",
    "    # 2. Strings: \"...\" or '...' -> \"STRING\"\n",
    "    code = re.sub(r'''(\".*?\"|'.*?')''', '\"STRING\"', code)\n",
    "    \n",
    "    # 3. Numbers: 123 -> 0\n",
    "    code = re.sub(r'\\b\\d+\\b', '0', code)\n",
    "    \n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313f97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_php_block(content: str) -> str:\n",
    "    match = re.search(r'<\\?php(.*?)\\?>', content, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce78181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(good_dir: str, bad_dir: str, php_only: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Load safe samples (from 'good' directory)\n",
    "    good_files = glob.glob(os.path.join(good_dir, \"*.php\"))\n",
    "    for filepath in tqdm(good_files, desc=\"Safe samples\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            cleaned = clean_php_code(content)\n",
    "            if php_only:\n",
    "                cleaned = extract_php_block(cleaned)\n",
    "            \n",
    "            if cleaned:\n",
    "                data.append({\n",
    "                    'filepath': filepath,\n",
    "                    'code': cleaned,\n",
    "                    'label': 0,\n",
    "                    'label_name': 'safe'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    # Load vulnerable samples (from 'bad' directory)\n",
    "    bad_files = glob.glob(os.path.join(bad_dir, \"*.php\"))\n",
    "    for filepath in tqdm(bad_files, desc=\"Vulnerable samples\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            cleaned = clean_php_code(content)\n",
    "            if php_only:\n",
    "                cleaned = extract_php_block(cleaned)\n",
    "            \n",
    "            if cleaned:\n",
    "                data.append({\n",
    "                    'filepath': filepath,\n",
    "                    'code': cleaned,\n",
    "                    'label': 1,\n",
    "                    'label_name': 'vulnerable'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Calculate hashes\n",
    "    df['code_hash'] = df['code'].apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n",
    "    df['struct_hash'] = df['code'].apply(lambda x: hashlib.md5(normalize_code(x).encode('utf-8')).hexdigest())\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7660df",
   "metadata": {},
   "source": [
    "## eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06432824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined dataset from: ./cache/php_balanced_realistic.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the NEW combined dataset (with safe patterns supplement)\n",
    "cache_path = \"./cache/php_balanced_realistic.pkl\"\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "    print(f\"ERROR: Combined dataset not found at {cache_path}\")\n",
    "    print(\"Please run: python code/step1_combine_data.py\")\n",
    "    raise FileNotFoundError(f\"Required dataset not found: {cache_path}\")\n",
    "\n",
    "print(f\"Loading combined dataset from: {cache_path}\")\n",
    "df = pd.read_pickle(cache_path)\n",
    "\n",
    "# Ensure 'label_name' column exists\n",
    "if 'label_name' not in df.columns:\n",
    "    df['label_name'] = df['label'].map({0: 'safe', 1: 'vulnerable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53ed8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "Total samples: 9,724\n",
      "\n",
      "Label distribution:\n",
      "label_name\n",
      "safe          4862\n",
      "vulnerable    4862\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance ratio: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label_name'].value_counts())\n",
    "print(f\"\\nClass balance ratio: {df['label'].value_counts()[0] / df['label'].value_counts()[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c92f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Length Statistics:\n",
      "----------------------------------------\n",
      "count    9724.000000\n",
      "mean      199.005348\n",
      "std       129.247928\n",
      "min        21.000000\n",
      "25%       102.000000\n",
      "50%       162.000000\n",
      "75%       258.000000\n",
      "max       614.000000\n",
      "Name: code_length, dtype: float64\n",
      "\n",
      "Approximate Token Count Statistics:\n",
      "----------------------------------------\n",
      "count    9724.000000\n",
      "mean       23.815508\n",
      "std        14.563140\n",
      "min         2.000000\n",
      "25%        12.000000\n",
      "50%        19.000000\n",
      "75%        34.000000\n",
      "max        69.000000\n",
      "Name: token_count_approx, dtype: float64\n",
      "\n",
      "============================================================\n",
      "SAMPLE SAFE CODE:\n",
      "============================================================\n",
      "$tainted = `cat /tmp/tainted.txt`;\n",
      "$tainted = (float) $tainted ;\n",
      "echo \"<div id='\".  $tainted .\"'>content</div>\" ;\n",
      "\n",
      "============================================================\n",
      "SAMPLE VULNERABLE CODE:\n",
      "============================================================\n",
      "<?php\n",
      "$input = $_GET['name'];\n",
      "$clean = addslashes($input);\n",
      "echo \"<div>\" . $clean . \"</div>\";\n",
      "?>\n"
     ]
    }
   ],
   "source": [
    "df['code_length'] = df['code'].apply(len)\n",
    "df['token_count_approx'] = df['code'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Code Length Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(df['code_length'].describe())\n",
    "\n",
    "print(\"\\nApproximate Token Count Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(df['token_count_approx'].describe())\n",
    "\n",
    "# Show sample code from each class\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE SAFE CODE:\")\n",
    "print(\"=\"*60)\n",
    "safe_sample = df[df['label'] == 0].iloc[0]['code']\n",
    "print(safe_sample[:500] + \"...\" if len(safe_sample) > 500 else safe_sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE VULNERABLE CODE:\")\n",
    "print(\"=\"*60)\n",
    "vuln_sample = df[df['label'] == 1].iloc[0]['code']\n",
    "print(vuln_sample[:500] + \"...\" if len(vuln_sample) > 500 else vuln_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494272a",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2274c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(config.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52494a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7080570bf2a74401a895dd8f9d1602bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing samples:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = min(1000, len(df))\n",
    "sample_df = df.sample(n=sample_size, random_state=config.RANDOM_SEED)\n",
    "\n",
    "token_lengths = []\n",
    "for code in tqdm(sample_df['code'], desc=\"Tokenizing samples\"):\n",
    "    tokens = tokenizer(code, truncation=False, add_special_tokens=True)\n",
    "    token_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "token_lengths = np.array(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e15c3053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Length Statistics (sample of 1000):\n",
      "----------------------------------------\n",
      "Min: 9\n",
      "Max: 245\n",
      "Mean: 77.8\n",
      "Median: 64.0\n",
      "95th percentile: 186.0\n",
      "99th percentile: 214.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nToken Length Statistics (sample of 1000):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Min: {token_lengths.min()}\")\n",
    "print(f\"Max: {token_lengths.max()}\")\n",
    "print(f\"Mean: {token_lengths.mean():.1f}\")\n",
    "print(f\"Median: {np.median(token_lengths):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(token_lengths, 95):.1f}\")\n",
    "print(f\"99th percentile: {np.percentile(token_lengths, 99):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52e814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples exceeding 512 tokens: 0.0%\n"
     ]
    }
   ],
   "source": [
    "truncation_rate = (token_lengths > config.MAX_LENGTH).mean() * 100\n",
    "print(f\"\\nSamples exceeding {config.MAX_LENGTH} tokens: {truncation_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f6afc",
   "metadata": {},
   "source": [
    "## Dataset Class and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a0da7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def augment_code(code):\n",
    "    \"\"\"Randomly rename variables to force model to learn logic.\"\"\"\n",
    "    # Find all variables like $tainted, $data, etc.\n",
    "    variables = set(re.findall(r'\\$[a-zA-Z_][a-zA-Z0-9_]*', code))\n",
    "    \n",
    "    # Don't rename superglobals\n",
    "    superglobals = {'$_GET', '$_POST', '$_COOKIE', '$_REQUEST', '$_SERVER', '$_FILES', '$GLOBALS'}\n",
    "    variables = variables - superglobals\n",
    "    \n",
    "    # Create random mapping\n",
    "    mapping = {}\n",
    "    for var in variables:\n",
    "        # Generate random name like $v_x7z\n",
    "        random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "        mapping[var] = f\"$v_{random_suffix}\"\n",
    "        \n",
    "    # Apply mapping\n",
    "    augmented_code = code\n",
    "    for var, new_name in mapping.items():\n",
    "        # Replace $var but not $var_2 (boundary check)\n",
    "        augmented_code = re.sub(re.escape(var) + r'(?![a-zA-Z0-9_])', new_name, augmented_code)\n",
    "        \n",
    "    return augmented_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee5547b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWE79Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, codes, labels, tokenizer, max_length, augment=False):\n",
    "        self.codes = codes\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        code = str(self.codes[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply augmentation (50% chance if enabled)\n",
    "        if self.augment and random.random() < 0.5:\n",
    "            code = augment_code(code)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c1ea6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED, stratify=df['label']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=config.VAL_SIZE, random_state=config.RANDOM_SEED, stratify=train_val_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfc1a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "  Training:   7,025 samples\n",
      "  Validation: 1,240 samples\n",
      "  Test:       1,459 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Training:   {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test:       {len(test_df):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34b05d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CWE79Dataset(train_df['code'].values, train_df['label'].values, tokenizer, config.MAX_LENGTH)\n",
    "val_dataset = CWE79Dataset(val_df['code'].values, val_df['label'].values, tokenizer, config.MAX_LENGTH)\n",
    "test_dataset = CWE79Dataset(test_df['code'].values, test_df['label'].values, tokenizer, config.MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bfed7",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db027488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c810943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME, num_labels=2, problem_type=\"single_label_classification\"\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e2f2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     124,647,170\n",
      "Frozen parameters:    109,879,296\n",
      "Trainable parameters: 14,767,874\n",
      "Training 11.8% of model\n"
     ]
    }
   ],
   "source": [
    "freeze_layers = 10\n",
    "\n",
    "# Freeze embeddings\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze first 10 transformer layers\n",
    "for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "    if i < freeze_layers:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Check trainable parameters\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen = total - trainable\n",
    "\n",
    "print(f\"Total parameters:     {total:,}\")\n",
    "print(f\"Frozen parameters:    {frozen:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Training {trainable/total*100:.1f}% of model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b97889",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "148eaf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1.0001423358917236, 0.9998576641082764]\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, average='binary'),\n",
    "        'recall': recall_score(labels, predictions, average='binary'),\n",
    "        'f1': f1_score(labels, predictions, average='binary'),\n",
    "    }\n",
    "\n",
    "# Handle class imbalance\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = torch.tensor([len(train_df) / (2 * c) for c in class_counts], dtype=torch.float).to(device)\n",
    "print(f\"Class weights: {class_weights.tolist()}\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    save_total_limit=3,\n",
    "    seed=config.RANDOM_SEED,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a18e4a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2d0c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba5ca43ee06430bbbb7016a26d59f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7019, 'learning_rate': 4.587155963302753e-06, 'epoch': 0.46}\n",
      "{'loss': 0.6915, 'learning_rate': 9.174311926605506e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711de82c6ecb489ca9a32eff550e84f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6805028915405273, 'eval_accuracy': 0.5532258064516129, 'eval_precision': 0.5733333333333334, 'eval_recall': 0.4161290322580645, 'eval_f1': 0.4822429906542056, 'eval_runtime': 19.9477, 'eval_samples_per_second': 62.162, 'eval_steps_per_second': 3.91, 'epoch': 0.99}\n",
      "{'loss': 0.6774, 'learning_rate': 1.3761467889908258e-05, 'epoch': 1.37}\n",
      "{'loss': 0.5408, 'learning_rate': 1.834862385321101e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b2578952944d229c9831bae12d92bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35471096634864807, 'eval_accuracy': 0.8362903225806452, 'eval_precision': 0.8183206106870229, 'eval_recall': 0.864516129032258, 'eval_f1': 0.8407843137254902, 'eval_runtime': 20.4696, 'eval_samples_per_second': 60.578, 'eval_steps_per_second': 3.811, 'epoch': 1.99}\n",
      "{'loss': 0.3461, 'learning_rate': 1.9673802242609582e-05, 'epoch': 2.28}\n",
      "{'loss': 0.2515, 'learning_rate': 1.9164118246687054e-05, 'epoch': 2.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dd27adb3d140c38a50b64ea1b613b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20694807171821594, 'eval_accuracy': 0.9185483870967742, 'eval_precision': 0.9625668449197861, 'eval_recall': 0.8709677419354839, 'eval_f1': 0.9144792548687553, 'eval_runtime': 31.9108, 'eval_samples_per_second': 38.858, 'eval_steps_per_second': 2.444, 'epoch': 2.99}\n",
      "{'loss': 0.211, 'learning_rate': 1.865443425076453e-05, 'epoch': 3.19}\n",
      "{'loss': 0.1931, 'learning_rate': 1.8144750254841998e-05, 'epoch': 3.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bba594087a4867a6d502616cc0adee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16265422105789185, 'eval_accuracy': 0.9314516129032258, 'eval_precision': 0.9768270944741533, 'eval_recall': 0.8838709677419355, 'eval_f1': 0.9280270956816258, 'eval_runtime': 36.5458, 'eval_samples_per_second': 33.93, 'eval_steps_per_second': 2.134, 'epoch': 4.0}\n",
      "{'loss': 0.1447, 'learning_rate': 1.763506625891947e-05, 'epoch': 4.1}\n",
      "{'loss': 0.1335, 'learning_rate': 1.7125382262996945e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c7e70d1f9d43f88235ad82525bfb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09464450925588608, 'eval_accuracy': 0.9612903225806452, 'eval_precision': 0.971947194719472, 'eval_recall': 0.95, 'eval_f1': 0.9608482871125612, 'eval_runtime': 35.2367, 'eval_samples_per_second': 35.191, 'eval_steps_per_second': 2.214, 'epoch': 5.0}\n",
      "{'loss': 0.1333, 'learning_rate': 1.6625891946992865e-05, 'epoch': 5.01}\n",
      "{'loss': 0.1043, 'learning_rate': 1.6116207951070337e-05, 'epoch': 5.46}\n",
      "{'loss': 0.1057, 'learning_rate': 1.560652395514781e-05, 'epoch': 5.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1f944a86d6465e8d8018562a467e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09458089619874954, 'eval_accuracy': 0.9548387096774194, 'eval_precision': 0.9763513513513513, 'eval_recall': 0.932258064516129, 'eval_f1': 0.9537953795379538, 'eval_runtime': 36.0845, 'eval_samples_per_second': 34.364, 'eval_steps_per_second': 2.162, 'epoch': 6.0}\n",
      "{'loss': 0.1046, 'learning_rate': 1.5096839959225283e-05, 'epoch': 6.37}\n",
      "{'loss': 0.0887, 'learning_rate': 1.4587155963302753e-05, 'epoch': 6.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9909fcb6b584bb1b3fe9626c16f7d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07421040534973145, 'eval_accuracy': 0.9701612903225807, 'eval_precision': 0.9755301794453507, 'eval_recall': 0.964516129032258, 'eval_f1': 0.9699918896999189, 'eval_runtime': 37.4354, 'eval_samples_per_second': 33.124, 'eval_steps_per_second': 2.084, 'epoch': 7.0}\n",
      "{'loss': 0.0763, 'learning_rate': 1.4077471967380225e-05, 'epoch': 7.28}\n",
      "{'loss': 0.0916, 'learning_rate': 1.3567787971457698e-05, 'epoch': 7.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41816b0d071c4e0ca100414883d44c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05636864900588989, 'eval_accuracy': 0.9733870967741935, 'eval_precision': 0.9726247987117552, 'eval_recall': 0.9741935483870968, 'eval_f1': 0.9734085414987913, 'eval_runtime': 37.5471, 'eval_samples_per_second': 33.025, 'eval_steps_per_second': 2.077, 'epoch': 8.0}\n",
      "{'loss': 0.0858, 'learning_rate': 1.305810397553517e-05, 'epoch': 8.19}\n",
      "{'loss': 0.0716, 'learning_rate': 1.254841997961264e-05, 'epoch': 8.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baed11e49fa0462fa1556ff61dc2b7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.049785859882831573, 'eval_accuracy': 0.9790322580645161, 'eval_precision': 0.9759615384615384, 'eval_recall': 0.9822580645161291, 'eval_f1': 0.9790996784565916, 'eval_runtime': 39.3971, 'eval_samples_per_second': 31.474, 'eval_steps_per_second': 1.98, 'epoch': 8.99}\n",
      "{'loss': 0.0686, 'learning_rate': 1.2038735983690114e-05, 'epoch': 9.1}\n",
      "{'loss': 0.0673, 'learning_rate': 1.1529051987767585e-05, 'epoch': 9.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08041f4787504d46b49e8403eb55cfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.046214405447244644, 'eval_accuracy': 0.9798387096774194, 'eval_precision': 0.9744816586921851, 'eval_recall': 0.9854838709677419, 'eval_f1': 0.9799518845228549, 'eval_runtime': 29.0256, 'eval_samples_per_second': 42.721, 'eval_steps_per_second': 2.687, 'epoch': 9.99}\n",
      "{'loss': 0.0577, 'learning_rate': 1.1019367991845057e-05, 'epoch': 10.01}\n",
      "{'loss': 0.0533, 'learning_rate': 1.0509683995922529e-05, 'epoch': 10.47}\n",
      "{'loss': 0.0569, 'learning_rate': 1e-05, 'epoch': 10.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803f5beffb8c45029b70977af8c2a320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039788804948329926, 'eval_accuracy': 0.9846774193548387, 'eval_precision': 0.9823434991974318, 'eval_recall': 0.9870967741935484, 'eval_f1': 0.9847144006436042, 'eval_runtime': 29.3619, 'eval_samples_per_second': 42.232, 'eval_steps_per_second': 2.657, 'epoch': 10.99}\n",
      "{'loss': 0.0497, 'learning_rate': 9.490316004077473e-06, 'epoch': 11.38}\n",
      "{'loss': 0.0532, 'learning_rate': 8.980632008154944e-06, 'epoch': 11.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4685819a1fe44a8d997c542cbfb722e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.034001607447862625, 'eval_accuracy': 0.9854838709677419, 'eval_precision': 0.9870550161812298, 'eval_recall': 0.9838709677419355, 'eval_f1': 0.9854604200323102, 'eval_runtime': 35.9784, 'eval_samples_per_second': 34.465, 'eval_steps_per_second': 2.168, 'epoch': 12.0}\n",
      "{'loss': 0.0517, 'learning_rate': 8.470948012232416e-06, 'epoch': 12.29}\n",
      "{'loss': 0.0382, 'learning_rate': 7.961264016309888e-06, 'epoch': 12.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32189f63b3fa4e76b5cd8b1edd7ad787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02894522435963154, 'eval_accuracy': 0.9854838709677419, 'eval_precision': 0.9839228295819936, 'eval_recall': 0.9870967741935484, 'eval_f1': 0.9855072463768116, 'eval_runtime': 37.3704, 'eval_samples_per_second': 33.181, 'eval_steps_per_second': 2.087, 'epoch': 13.0}\n",
      "{'loss': 0.0333, 'learning_rate': 7.45158002038736e-06, 'epoch': 13.2}\n",
      "{'loss': 0.0338, 'learning_rate': 6.941896024464833e-06, 'epoch': 13.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7340a4ec86a4dfdb9ae3ffaf909ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.028582246974110603, 'eval_accuracy': 0.9854838709677419, 'eval_precision': 0.9854838709677419, 'eval_recall': 0.9854838709677419, 'eval_f1': 0.9854838709677419, 'eval_runtime': 34.3893, 'eval_samples_per_second': 36.058, 'eval_steps_per_second': 2.268, 'epoch': 14.0}\n",
      "{'loss': 0.0365, 'learning_rate': 6.432212028542304e-06, 'epoch': 14.11}\n",
      "{'loss': 0.0332, 'learning_rate': 5.922528032619776e-06, 'epoch': 14.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92037b7ba03b4e578695598806be78b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02915828675031662, 'eval_accuracy': 0.9870967741935484, 'eval_precision': 0.9870967741935484, 'eval_recall': 0.9870967741935484, 'eval_f1': 0.9870967741935484, 'eval_runtime': 24.73, 'eval_samples_per_second': 50.141, 'eval_steps_per_second': 3.154, 'epoch': 15.0}\n",
      "{'loss': 0.0329, 'learning_rate': 5.412844036697248e-06, 'epoch': 15.02}\n",
      "{'loss': 0.029, 'learning_rate': 4.90316004077472e-06, 'epoch': 15.47}\n",
      "{'loss': 0.0271, 'learning_rate': 4.393476044852192e-06, 'epoch': 15.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286303d1b5ac42efbc03a786d4ce3fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02873540110886097, 'eval_accuracy': 0.9854838709677419, 'eval_precision': 0.9839228295819936, 'eval_recall': 0.9870967741935484, 'eval_f1': 0.9855072463768116, 'eval_runtime': 24.9886, 'eval_samples_per_second': 49.623, 'eval_steps_per_second': 3.121, 'epoch': 16.0}\n",
      "{'loss': 0.0233, 'learning_rate': 3.8837920489296635e-06, 'epoch': 16.38}\n",
      "{'loss': 0.0264, 'learning_rate': 3.3741080530071357e-06, 'epoch': 16.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9eec30560e4f298e018eb230f96df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026798125356435776, 'eval_accuracy': 0.9854838709677419, 'eval_precision': 0.9854838709677419, 'eval_recall': 0.9854838709677419, 'eval_f1': 0.9854838709677419, 'eval_runtime': 35.295, 'eval_samples_per_second': 35.132, 'eval_steps_per_second': 2.21, 'epoch': 16.99}\n",
      "{'train_runtime': 4937.5328, 'train_samples_per_second': 28.456, 'train_steps_per_second': 0.442, 'train_loss': 0.14822749438359895, 'epoch': 16.99}\n"
     ]
    }
   ],
   "source": [
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = torch.tensor(\n",
    "    [len(train_df) / (2 * c) for c in class_counts], dtype=torch.float\n",
    ").to(device)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,  \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "072e3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef10c17515c4904a191ea182e7f2043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9868\n",
      "Precision: 0.9862\n",
      "Recall:    0.9835\n",
      "F1-Score:  0.9848\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Safe       0.99      0.99      0.99       936\n",
      "  Vulnerable       0.99      0.98      0.98       726\n",
      "\n",
      "    accuracy                           0.99      1662\n",
      "   macro avg       0.99      0.99      0.99      1662\n",
      "weighted avg       0.99      0.99      0.99      1662\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[926  10]\n",
      " [ 12 714]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = test_df['label'].values\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy_score(labels, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "print(f\"Recall:    {recall_score(labels, preds):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(labels, preds):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=['Safe', 'Vulnerable']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c4d4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ./php_cwe79_codebert_output/best_model\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './cwe79_codebert_output/best_model'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# To load later:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaForSequenceClassification, RobertaTokenizer\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cwe79_codebert_output/best_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cwe79_codebert_output/best_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2600\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2599\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 2600\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2605\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2608\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2609\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2614\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m    104\u001b[0m ):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The name cannot start or end with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and the maximum length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './cwe79_codebert_output/best_model'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "# Already saved at:\n",
    "print(f\"Model saved at: {config.OUTPUT_DIR}/best_model\")\n",
    "\n",
    "# To load later:\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./cwe79_codebert_output/best_model\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./cwe79_codebert_output/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98ef57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df77972",
   "metadata": {},
   "source": [
    "# javascript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed13aa2",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b3b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d09fc",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5f0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_DIR = \"d:/ai_codebert_fp\"\n",
    "    SAFE_DIR = os.path.join(BASE_DIR, \"dataset\", \"javascript\", \"CWE_79\", \"safe\")\n",
    "    UNSAFE_DIR = os.path.join(BASE_DIR, \"dataset\", \"javascript\", \"CWE_79\", \"unsafe\")\n",
    "    CACHE_DIR = os.path.join(BASE_DIR, \"dataset\", \"cache\")\n",
    "    OUTPUT_DIR = \"./js_xss_codebert_output\"\n",
    "    MODEL_NAME = \"microsoft/codebert-base\"\n",
    "    MAX_LENGTH = 512\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EPOCHS = 5\n",
    "    SEED = 42\n",
    "    TEST_SIZE = 0.15\n",
    "    VAL_SIZE = 0.15\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31f991",
   "metadata": {},
   "source": [
    "## data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75dbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_js_code(content: str) -> str:\n",
    "    \n",
    "    # 1. Remove the header comment block (contains \"Safe sample\" / \"Unsafe sample\")\n",
    "    content = re.sub(r'/\\*\\s*(Safe|Unsafe)\\s+sample.*?\\*/', '', content, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # 2. Remove MIT License block\n",
    "    content = re.sub(r'/\\*\\s*MIT License.*?\\*/', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # 3. Remove //flaw marker (CRITICAL - this is a direct label leak!)\n",
    "    content = re.sub(r'//\\s*flaw\\s*\\n?', '', content)\n",
    "    \n",
    "    # 4. Remove //no_sanitizing marker\n",
    "    content = re.sub(r'//\\s*no_sanitizing\\s*\\n?', '', content)\n",
    "    \n",
    "    # 5. Remove any remaining multi-line comments that might contain hints\n",
    "    # But keep inline comments that might be part of real code\n",
    "    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # 6. Clean up excessive whitespace\n",
    "    content = re.sub(r'\\n\\s*\\n+', '\\n\\n', content.strip())\n",
    "    \n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b16b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "/* \n",
      "Safe sample\n",
      "input : reads the field UserData from the variable $_GET\n",
      "sanitize : cast in float\n",
      "*/...\n",
      "\n",
      "After cleaning:\n",
      "var x = 1;\n",
      "var y = 2;\n"
     ]
    }
   ],
   "source": [
    "test_code = \"\"\"/* \n",
    "Safe sample\n",
    "input : reads the field UserData from the variable $_GET\n",
    "sanitize : cast in float\n",
    "*/\n",
    "/*\n",
    "MIT License\n",
    "Copyright (c) 2021 MAUREL Hlose\n",
    "...\n",
    "*/\n",
    "var x = 1;\n",
    "//flaw\n",
    "//no_sanitizing\n",
    "var y = 2;\n",
    "\"\"\"\n",
    "print(\"Before cleaning:\")\n",
    "print(test_code[:100] + \"...\")\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(clean_js_code(test_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5c564",
   "metadata": {},
   "source": [
    "## Normalize Code for Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b388b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"Normalize code to detect structural duplicates.\"\"\"\n",
    "    # Remove comments\n",
    "    code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    # Normalize whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code)\n",
    "    # Normalize variable names\n",
    "    code = re.sub(r'\\b(var|let|const)\\s+[a-zA-Z_$][a-zA-Z0-9_$]*', r'\\1 VAR', code)\n",
    "    return code.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8acdb",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d028da",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7400c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JS dataset from: ./cache/js_training_data_v3.pkl\n",
      "\n",
      "Dataset: 14031 | Safe: 6957 | Unsafe: 7074\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(safe_dir, unsafe_dir):\n",
    "    \"\"\"Load JavaScript files with proper preprocessing to prevent label leakage.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Load safe samples\n",
    "    for filepath in tqdm(glob.glob(os.path.join(safe_dir, \"*.js\")), desc=\"Loading safe\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                raw_code = f.read()\n",
    "                # CRITICAL: Clean the code to remove label leaks!\n",
    "                cleaned_code = clean_js_code(raw_code)\n",
    "                if cleaned_code and len(cleaned_code) > 20:  # Skip if too short after cleaning\n",
    "                    data.append({'code': cleaned_code, 'label': 0, 'source': 'original'})\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Load unsafe samples\n",
    "    for filepath in tqdm(glob.glob(os.path.join(unsafe_dir, \"*.js\")), desc=\"Loading unsafe\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                raw_code = f.read()\n",
    "                # CRITICAL: Clean the code to remove label leaks!\n",
    "                cleaned_code = clean_js_code(raw_code)\n",
    "                if cleaned_code and len(cleaned_code) > 20:\n",
    "                    data.append({'code': cleaned_code, 'label': 1, 'source': 'original'})\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['struct_hash'] = df['code'].apply(lambda x: hashlib.md5(normalize_code(x).encode()).hexdigest())\n",
    "    return df\n",
    "\n",
    "\n",
    "cache_path = \"./cache/js_training_data_v3.pkl\"\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "    print(f\"ERROR: JS dataset not found at {cache_path}\")\n",
    "    print(\"Please run: python code/step1_combine_js_data.py\")\n",
    "    raise FileNotFoundError(f\"Required dataset not found: {cache_path}\")\n",
    "\n",
    "print(f\"Loading JS dataset from: {cache_path}\")\n",
    "df = pd.read_pickle(cache_path)\n",
    "\n",
    "if 'label_name' not in df.columns:\n",
    "    df['label_name'] = df['label'].map({0: 'safe', 1: 'vulnerable'})\n",
    "\n",
    "print(f\"\\nDataset: {len(df)} | Safe: {len(df[df['label']==0])} | Unsafe: {len(df[df['label']==1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c057f0a",
   "metadata": {},
   "source": [
    "## Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847547c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "  Total: 14031\n",
      "  Safe: 6957\n",
      "  Vulnerable: 7074\n",
      "\n",
      "By source:\n",
      "source\n",
      "original_cwe79         10920\n",
      "safe_dompurify           221\n",
      "vuln_dom_xss             200\n",
      "safe_react               200\n",
      "safe_dom                 199\n",
      "vuln_dom_innerhtml       192\n",
      "safe_textcontent         150\n",
      "safe_encoding            144\n",
      "vuln_express_xss         140\n",
      "vuln_express_query       138\n",
      "safe_validation          127\n",
      "vuln_express_body        122\n",
      "vuln_eval_xss            120\n",
      "safe_html_escape         118\n",
      "vuln_url_xss             116\n",
      "vuln_eval                100\n",
      "vuln_ajax_xss             99\n",
      "vuln_attr_xss             99\n",
      "vuln_location             83\n",
      "vuln_express_full         79\n",
      "safe_url_encode           75\n",
      "vuln_express_params       74\n",
      "vuln_react_xss            74\n",
      "vuln_react                71\n",
      "vuln_template_xss         66\n",
      "safe_template             55\n",
      "safe_json                 40\n",
      "vuln_realistic             9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Splits:\n",
      "  Train: 10137 | Val: 1789 | Test: 2105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset:\")\n",
    "print(f\"  Total: {len(df)}\")\n",
    "print(f\"  Safe: {len(df[df['label']==0])}\")\n",
    "print(f\"  Vulnerable: {len(df[df['label']==1])}\")\n",
    "print(f\"\\nBy source:\")\n",
    "print(df['source'].value_counts())\n",
    "\n",
    "# Proper 3-way split\n",
    "train_val_df, test_df = train_test_split(df, test_size=config.TEST_SIZE, random_state=config.SEED, stratify=df['label'])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=config.VAL_SIZE, random_state=config.SEED, stratify=train_val_df['label'])\n",
    "\n",
    "print(f\"\\nSplits:\")\n",
    "print(f\"  Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe0a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10137 | Val: 1789 | Test: 2105\n"
     ]
    }
   ],
   "source": [
    "train_val_df, test_df = train_test_split(df, test_size=config.TEST_SIZE, random_state=config.SEED, stratify=df['label'])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=config.VAL_SIZE, random_state=config.SEED, stratify=train_val_df['label'])\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae752a",
   "metadata": {},
   "source": [
    "## Dataset Class and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14eedfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: Train=10137, Val=1789, Test=2105\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        code = str(self.data.loc[idx, 'code'])\n",
    "        label = int(self.data.loc[idx, 'label'])\n",
    "        enc = self.tokenizer(code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze(), 'labels': torch.tensor(label)}\n",
    "\n",
    "train_dataset = XSSDataset(train_df, tokenizer, config.MAX_LENGTH)\n",
    "val_dataset = XSSDataset(val_df, tokenizer, config.MAX_LENGTH)\n",
    "test_dataset = XSSDataset(test_df, tokenizer, config.MAX_LENGTH)\n",
    "\n",
    "print(f\"Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2ff20",
   "metadata": {},
   "source": [
    "## Model with Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "defa49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 14,767,874 / 124,647,170 (11.8%)\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=2)\n",
    "\n",
    "# Freeze all except last 2 layers + classifier\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.roberta.encoder.layer[:-2]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({trainable/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648b82f",
   "metadata": {},
   "source": [
    "## Weighted Trainer for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623649c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1.0084559917449951, 0.99168461561203]\n"
     ]
    }
   ],
   "source": [
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = torch.tensor([len(train_df) / (2 * c) for c in class_counts], dtype=torch.float).to(device)\n",
    "print(f\"Class weights: {class_weights.tolist()}\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = torch.nn.CrossEntropyLoss(weight=self.class_weights)(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    return {'accuracy': accuracy_score(labels, preds), 'precision': precision_score(labels, preds), \n",
    "            'recall': recall_score(labels, preds), 'f1': f1_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269f4d2",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e62a03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    save_total_limit=3,\n",
    "    seed=config.SEED,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580678c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c80f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67694e3427704ebea985b57de341bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6953, 'learning_rate': 6.289308176100629e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6968, 'learning_rate': 1.2578616352201259e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6812, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4466, 'learning_rate': 1.9424964936886398e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3033, 'learning_rate': 1.8723702664796635e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2881, 'learning_rate': 1.8022440392706875e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dbff0ff60544028b632f2f782d7ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3021243214607239, 'eval_accuracy': 0.8714365567356065, 'eval_precision': 0.9772727272727273, 'eval_recall': 0.7627494456762749, 'eval_f1': 0.8567870485678705, 'eval_runtime': 48.3044, 'eval_samples_per_second': 37.036, 'eval_steps_per_second': 1.159, 'epoch': 1.0}\n",
      "{'loss': 0.2565, 'learning_rate': 1.732117812061711e-05, 'epoch': 1.1}\n",
      "{'loss': 0.24, 'learning_rate': 1.661991584852735e-05, 'epoch': 1.26}\n",
      "{'loss': 0.212, 'learning_rate': 1.5918653576437588e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1922, 'learning_rate': 1.5231416549789621e-05, 'epoch': 1.58}\n",
      "{'loss': 0.193, 'learning_rate': 1.4530154277699861e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1525, 'learning_rate': 1.38288920056101e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e8684bc2834d388d4cbee6090bffce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12390203028917313, 'eval_accuracy': 0.9530463946338736, 'eval_precision': 0.9845971563981043, 'eval_recall': 0.9212860310421286, 'eval_f1': 0.9518900343642611, 'eval_runtime': 31.6476, 'eval_samples_per_second': 56.529, 'eval_steps_per_second': 1.769, 'epoch': 2.0}\n",
      "{'loss': 0.14, 'learning_rate': 1.3127629733520338e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1464, 'learning_rate': 1.2426367461430575e-05, 'epoch': 2.21}\n",
      "{'loss': 0.1499, 'learning_rate': 1.1725105189340815e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1212, 'learning_rate': 1.1023842917251053e-05, 'epoch': 2.52}\n",
      "{'loss': 0.1438, 'learning_rate': 1.0322580645161291e-05, 'epoch': 2.68}\n",
      "{'loss': 0.1253, 'learning_rate': 9.62131837307153e-06, 'epoch': 2.84}\n",
      "{'loss': 0.1444, 'learning_rate': 8.920056100981768e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c1a48a7ac345bfb8a64498580ea63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09398985654115677, 'eval_accuracy': 0.9591950810508664, 'eval_precision': 0.9726339794754846, 'eval_recall': 0.9456762749445676, 'eval_f1': 0.9589657110736369, 'eval_runtime': 32.1697, 'eval_samples_per_second': 55.611, 'eval_steps_per_second': 1.741, 'epoch': 3.0}\n",
      "{'loss': 0.1212, 'learning_rate': 8.218793828892006e-06, 'epoch': 3.15}\n",
      "{'loss': 0.1439, 'learning_rate': 7.5175315568022445e-06, 'epoch': 3.31}\n",
      "{'loss': 0.106, 'learning_rate': 6.816269284712484e-06, 'epoch': 3.47}\n",
      "{'loss': 0.1041, 'learning_rate': 6.115007012622721e-06, 'epoch': 3.63}\n",
      "{'loss': 0.1233, 'learning_rate': 5.41374474053296e-06, 'epoch': 3.79}\n",
      "{'loss': 0.119, 'learning_rate': 4.712482468443198e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a740a69c0c3458494463547a23a7edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09985590726137161, 'eval_accuracy': 0.9580771380659586, 'eval_precision': 0.9858989424206815, 'eval_recall': 0.9301552106430155, 'eval_f1': 0.9572162007986309, 'eval_runtime': 32.5863, 'eval_samples_per_second': 54.9, 'eval_steps_per_second': 1.719, 'epoch': 4.0}\n",
      "{'loss': 0.1182, 'learning_rate': 4.011220196353436e-06, 'epoch': 4.1}\n",
      "{'loss': 0.1245, 'learning_rate': 3.309957924263675e-06, 'epoch': 4.26}\n",
      "{'loss': 0.1099, 'learning_rate': 2.6086956521739132e-06, 'epoch': 4.42}\n",
      "{'loss': 0.1029, 'learning_rate': 1.9074333800841516e-06, 'epoch': 4.57}\n",
      "{'loss': 0.1209, 'learning_rate': 1.2061711079943899e-06, 'epoch': 4.73}\n",
      "{'loss': 0.111, 'learning_rate': 5.049088359046283e-07, 'epoch': 4.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed328bea5a0b4440842fdcade6aaa410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08771800994873047, 'eval_accuracy': 0.961430967020682, 'eval_precision': 0.983739837398374, 'eval_recall': 0.9390243902439024, 'eval_f1': 0.9608621667612025, 'eval_runtime': 32.6596, 'eval_samples_per_second': 54.777, 'eval_steps_per_second': 1.715, 'epoch': 5.0}\n",
      "{'train_runtime': 1486.5615, 'train_samples_per_second': 34.095, 'train_steps_per_second': 1.066, 'train_loss': 0.21472310433252376, 'epoch': 5.0}\n",
      "Model saved to ./js_xss_codebert_output/best_model\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(f\"{config.OUTPUT_DIR}/best_model\")\n",
    "tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/best_model\")\n",
    "print(f\"Model saved to {config.OUTPUT_DIR}/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2beeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on TEST set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1e0ecb09ef487a947c69a511bad083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9468\n",
      "Precision: 0.9731\n",
      "Recall:    0.9199\n",
      "F1-Score:  0.9457\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Safe       0.92      0.97      0.95      1044\n",
      "  Vulnerable       0.97      0.92      0.95      1061\n",
      "\n",
      "    accuracy                           0.95      2105\n",
      "   macro avg       0.95      0.95      0.95      2105\n",
      "weighted avg       0.95      0.95      0.95      2105\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1017   27]\n",
      " [  85  976]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on TEST set...\")\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = test_df['label'].values\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy_score(labels, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "print(f\"Recall:    {recall_score(labels, preds):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(labels, preds):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=['Safe', 'Vulnerable']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
